{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f934e389",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from fake_useragent import UserAgent\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import urllib.request as ur\n",
    "\n",
    "\n",
    "def search_information(name,page_n):\n",
    "\n",
    "    df_dic = {}\n",
    "    dic_keys = ['title']\n",
    "    \n",
    "    url = 'https://github.com/orgs/{}/repositories?page={}'.format(name,page_n)\n",
    "\n",
    "    html = ur.urlopen(url)\n",
    "    soup = bs(html.read(), \"html.parser\")\n",
    "\n",
    "    # 이름,별점,블로그,방문자 리뷰\n",
    "    repository_list = soup.find_all(\"li\", \"Box-row\") # 제목\n",
    "    # print(len(repository_list)) # 한페이지당 30개의 repository가 존재 (full일 때)\n",
    "\n",
    "\n",
    "    for k in dic_keys:\n",
    "        df_dic[k] = []\n",
    "\n",
    "    for repo in repository_list:\n",
    "        title = repo.find('a','d-inline-block').text.strip() # 각 repository별 title 추출\n",
    "\n",
    "\n",
    "        df_dic['title'].append(title)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    return pd.DataFrame(df_dic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "165fa35d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(308, 1)\n",
      "14.698469161987305  second\n"
     ]
    }
   ],
   "source": [
    "def main(name):\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "            columns=[\n",
    "                \"title\",\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    for i in range(1,16): # 한 페이지당 repo 30개씩 추출 가능 \n",
    "        temp_df = search_information(name,i) #search_information 함수 사용\n",
    "\n",
    "        df = pd.concat([df,temp_df],ignore_index=True) #여러개의 동일한 형태 DataFrame 합치기 -> 각 페이지 데이터프레임 형태 합침\n",
    "    return df\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    start = time.time()\n",
    "    result_df = main('NVIDIA')\n",
    "    end = time.time()\n",
    "    print(result_df.shape)\n",
    "    print(end - start, \" second\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "559486bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spark-rapids-jni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spark-rapids-benchmarks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DALI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NeMo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cheminformatics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>scenejs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>tdl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>cdt-nsight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>ptp-nsight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>gef-nsight</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>308 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       title\n",
       "0           spark-rapids-jni\n",
       "1    spark-rapids-benchmarks\n",
       "2                       DALI\n",
       "3                       NeMo\n",
       "4            cheminformatics\n",
       "..                       ...\n",
       "303                  scenejs\n",
       "304                      tdl\n",
       "305               cdt-nsight\n",
       "306               ptp-nsight\n",
       "307               gef-nsight\n",
       "\n",
       "[308 rows x 1 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8dae0785",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from urllib.request import urlopen\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time  # 시간 데이터 다루기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a0a2a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning time :  137.97776794433594\n"
     ]
    }
   ],
   "source": [
    "# https://api.github.com/repos/nvidia/nemo\n",
    "from collections import defaultdict #딕셔너리를 만드는 dict클래스의 서브 클래스\n",
    "\n",
    "urls = [[i,f\"https://api.github.com/repos/nvidia/{i}\"] for i in result_df['title']]\n",
    "dic = defaultdict(str)\n",
    "\n",
    "# PAT(Personnal Access Token)을 발급받아 1시간에 60개에서 5000개로 요청한도를 늘린다\n",
    "headers = { \"Authorization\": \"token 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\"} \n",
    "s = time.time()\n",
    "\n",
    "for i,url in urls:\n",
    "    response = requests.get(url, headers=headers)\n",
    "    create_date = response.json().get('created_at')\n",
    "    dic[i] = create_date\n",
    "#     print(dic[i])\n",
    "print('learning time : ',time.time()-s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "66b817c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2012-05-10T22:29:32Z'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1beeddb2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "If using all scalar values, you must pass an index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdic\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m df\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/crawling/lib/python3.9/site-packages/pandas/core/frame.py:636\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    630\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[1;32m    631\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[1;32m    632\u001b[0m     )\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    635\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[0;32m--> 636\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[1;32m    638\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmrecords\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmrecords\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/crawling/lib/python3.9/site-packages/pandas/core/internals/construction.py:502\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    494\u001b[0m     arrays \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    495\u001b[0m         x\n\u001b[1;32m    496\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x\u001b[38;5;241m.\u001b[39mdtype, ExtensionDtype)\n\u001b[1;32m    497\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m x\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    498\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays\n\u001b[1;32m    499\u001b[0m     ]\n\u001b[1;32m    500\u001b[0m     \u001b[38;5;66;03m# TODO: can we get rid of the dt64tz special case above?\u001b[39;00m\n\u001b[0;32m--> 502\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/crawling/lib/python3.9/site-packages/pandas/core/internals/construction.py:120\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 120\u001b[0m         index \u001b[38;5;241m=\u001b[39m \u001b[43m_extract_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    122\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/crawling/lib/python3.9/site-packages/pandas/core/internals/construction.py:664\u001b[0m, in \u001b[0;36m_extract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    661\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPer-column arrays must each be 1-dimensional\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    663\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m indexes \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m raw_lengths:\n\u001b[0;32m--> 664\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf using all scalar values, you must pass an index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m have_series:\n\u001b[1;32m    667\u001b[0m     index \u001b[38;5;241m=\u001b[39m union_indexes(indexes)\n",
      "\u001b[0;31mValueError\u001b[0m: If using all scalar values, you must pass an index"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11609b2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Crawling",
   "language": "python",
   "name": "crawling"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
